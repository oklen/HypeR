
# configuration groups
defaults:
  - encoder: hf_bert
  - train: biencoder_default
  - datasets: encoder_train_default
  # - dis_server: server


train_datasets:
dev_datasets:
output_dir: /relevance2-nfs/zefeng/old_try/web-t5/outputs
train_sampling_rates:
loss_scale_factors:

output_folder: /relevance2-nfs/zefeng/web-t5/predictions/bm25
model_name: dpr
model_configuration: None
test_config: "/relevance2-nfs/zefeng/web-t5/conf/test_data.json"
logdir: "/relevance2-nfs/zefeng/old_try/web-t5/logs/ranking/"

#cp from retriever
dummy_encoded_ctx_file: /home/v-zefengcai/models/dummy_kilt_passages_2048_0.pkl
encoded_ctx_file: /home/v-zefengcai/models/kilt_passages_2048_0.pkl
ctx_file: /home/v-zefengcai/models/kilt_w100_title.tsv
KILT_mapping: /home/v-zefengcai/models/mapping_KILT_title.p

hnsw_index: False

# Whether to lower case the input text. Set True for uncased models, False for the cased ones.
do_lower_case: True

val_av_rank_start_epoch: 30
seed: 42
checkpoint_file_name: dpr_biencoder

# A trained bi-encoder checkpoint file to initialize the model
# model_file: /home/v-zefengcai/models/dpr_multi_set_hf_bert.0
model_file: /home/v-zefengcai/models/dpr_multi_set_hf_bert.0
# TODO: move to a conf group
# local_rank for distributed training on gpus

# TODO: rename to distributed_rank
local_rank: -1
global_loss_buf_sz: 592000
device:
distributed_world_size:
distributed_port:
distributed_init_method:

no_cuda: False
n_gpu: 8
fp16: False
n_tpu_cores: 0
batch_size: 30 # DPR query size
train_batch: 30 #Use this!
eval_batch: 84
debug_mode: False
profile_mode: False
n_docs: 100
local_process: True
ng_count: 5 # Use Top-k negative samples

num_sanity_val_steps: 0

# For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']."
#        "See details at https://nvidia.github.io/apex/amp.html
fp16_opt_level: O1

# tokens which won't be slit by tokenizer
special_tokens:

ignore_checkpoint_offset: False
ignore_checkpoint_optimizer: False
ignore_checkpoint_lr: False

# set to >1 to enable multiple query encoders
multi_q_encoder: False

# Set to True to reduce memory footprint and loose a bit the full train data randomization if you train in DDP mode
local_shards_dataloader: False

# Additional argument for Downstream tasks
repetition_penalty: 1.0
max_grad_norm: 1.0
do_train: True
do_predict: False
gradient_accumulation_steps: 1
data_dir: /relevance2-nfs/zefeng/web-t5/data
# model_name_or_path: facebook/bart-base
model_name_or_path: t5-base
# model_name_or_path: facebook/bart-base
learning_rate: 2e-5
module_learning_rate: 5e-4
reduce_loss: None


# Required in base_transformers.py
# weight_decay: 0.01
weight_decay: 0.01
adam_epsilon: 1e-8
num_train_epochs: 20
warmup_steps: 100

# Do not use additional worker in training
num_workers_training: 10

# To set up the RAG and decoding
# cache_dir: ./output
max_sequence_length: 512
num_beams: 1
num_beam_groups: 1
max_length: 10
min_length: 1
use_cache: True
num_return_sequences: 1
remove_invalid_values: False
exponential_decay_length_penalty: None
num_train_epoch: 20
early_stopping: True
length_penalty: 1.0
label_smoothing: 0.01
use_prompt: False
quantization_factor: 100
retrieve_mode: True
dense_model: False

selected_gpu: False # Wheter select specificed gpu
# gpu_list: '1,2,3,4,5'
# setup used dataset for finetuning
dataset: aidayago2
# dataset: fever
# negative_samples_dir: /relevance2-nfs/zefeng/web-t5/predictions/bm25/
# sparse_negative_samples_dir: /relevance2-nfs/zefeng/web-t5/predictions/bm25/
negative_samples_dir: /relevance2-nfs/zefeng/web-t5/predictions/sparse_ng/
sparse_negative_samples_dir: /relevance2-nfs/zefeng/web-t5/predictions/sparse_ng/
# negative_samples_dir: /relevance2-nfs/zefeng/web-t5/predictions/dense_ng
# negative_samples_dir: /relevance2-nfs/zefeng/web-t5/predictions/sparse_ng
# sparse_negative_samples_dir: /relevance2-nfs/zefeng/web-t5/predictions/bm25/

last_vec_path: /relevance2-nfs/zefeng/old_try/web-t5/outputs/last_vec_path.pkl
last_vec_path_dev: /relevance2-nfs/zefeng/old_try/web-t5/outputs/last_vec_path_dev.pkl # Sparse use only!

last_sparse_output: /relevance2-nfs/zefeng/old_try/web-t5/outputs/last_sparse.txt
last_sparse_output_dev: /relevance2-nfs/zefeng/old_try/web-t5/outputs/last_sparse_dev.txt

retrieve_config: /relevance2-nfs/zefeng/web-t5/conf/train_data.json
anserini_path: /relevance2-nfs/zefeng/anserini
title_to_text_path: /home/v-zefengcai/models/title_to_path.pkl
save_index_dir: /relevance2-nfs/zefeng/anserini/indexes/lucene-index.msmarco-passage-distill-splade-max
model_type: bert-base-uncased
sparse_id_map: /relevance2-nfs/zefeng/web-t5/outputs/sparse_id_map.lines
hits: 100
topicreader: 'TsvString'


dis_faiss: True
# reload_dataloaders_every_n_epochs: 2
parallelism: 90
use_dpr: True
check_val_every_n_epoch: 1
total_steps: 50000
freeze_d_model: True
emb_size: 768
down_emb_size: 128
attempt_mode: False
load_question_name:
load_ctx_name:
use_attempt: False
used_dataset: 'aidayago2,fever,hotpotqa,nq,structured_zeroshot,triviaqa,wow'
freeze_lm_head: True
do_val_on:
use_v2: False
share_prompt_length: 100
num_workers: 8
js_weight: 0.0001
mpt_mode: False
use_prefix: False
prefix_hidden_size: 100
n_tokens: 100
customer_keys:
share_expand: 5
short_cut_mode: True
beir_mode: False
few_shot_data_cnt: -1
atten_weight_record: False
fixed_temp_step: 0
topn: -1